{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JguZ3NBZUkpo"
   },
   "source": [
    "# Convolutional Neural Networks (convnets or CNNs) with tf.keras and tf.data.\n",
    "\n",
    "For this notebook we will look at a convnet problem for image classification. Like the first two tutorials, we will classify Fashion-MNIST digits. This tutorial was inspired by [this one](https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/5.1-introduction-to-convnets.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "qWT31nsxURdJ"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "vNCKRyhQVSWO"
   },
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "5eDlt-AnVafg"
   },
   "outputs": [],
   "source": [
    "TRAINING_SIZE = len(train_images)\n",
    "TEST_SIZE = len(test_images)\n",
    "\n",
    "train_images = np.asarray(train_images, dtype=np.float32) / 255\n",
    "\n",
    "# Convert the train images and add channels\n",
    "train_images = train_images.reshape((TRAINING_SIZE, 28, 28, 1))\n",
    "\n",
    "test_images = np.asarray(test_images, dtype=np.float32) / 255\n",
    "# Convert the train images and add channels\n",
    "test_images = test_images.reshape((TEST_SIZE, 28, 28, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "YNi4TNvlVcIB"
   },
   "outputs": [],
   "source": [
    "# How many categories we are predicting from (0-9)\n",
    "LABEL_DIMENSIONS = 10\n",
    "\n",
    "train_labels  = tf.keras.utils.to_categorical(train_labels, LABEL_DIMENSIONS)\n",
    "test_labels = tf.keras.utils.to_categorical(test_labels, LABEL_DIMENSIONS)\n",
    "\n",
    "# Cast the labels to floats, needed later\n",
    "train_labels = train_labels.astype(np.float32)\n",
    "test_labels = test_labels.astype(np.float32)\n",
    "\n",
    "class_names = [\n",
    "    'T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt',\n",
    "    'Sneaker', 'Bag', 'Ankle boot'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "iHaND5a6Vc-B"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 3, 3, 64)          36928     \n",
      "=================================================================\n",
      "Total params: 55,744\n",
      "Trainable params: 55,744\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential()\n",
    "\n",
    "model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation=tf.nn.relu, input_shape=(28, 28, 1)))\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=2))\n",
    "model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation=tf.nn.relu))\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=2))\n",
    "model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation=tf.nn.relu))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see above that the output of every convolutional layer is a 3D tensor of shape (`height`, `width`, `filters`). The width and height tend to get smaller as we go deeper into the network and the number of filters or channels increases from the input channel size of 1. \n",
    "\n",
    "The last part of the network for the classification task is similar to the other notebooks and consists of `Dense` layers which process 1D vectors. So we first need to `Flatten` our 3D outputs from the convolutional part to 1D and then add the `Dense` layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "8EOBhY6xVd1S"
   },
   "outputs": [],
   "source": [
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(64, activation=tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(LABEL_DIMENSIONS, activation=tf.nn.softmax))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the network is again similar to all the previous notebooks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "SVUAOoeXVfQA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 3, 3, 64)          36928     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 576)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                36928     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 93,322\n",
      "Trainable params: 93,322\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "8jpqP9JYVgxM"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE=128\n",
    "\n",
    "# Because tf.data may work with potentially **large** collections of data\n",
    "# we do not shuffle the entire dataset by default\n",
    "# Instead, we maintain a buffer of SHUFFLE_SIZE elements\n",
    "# and sample from there.\n",
    "SHUFFLE_SIZE = 10000 \n",
    "\n",
    "# Create the dataset\n",
    "training_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
    "training_dataset = training_dataset.shuffle(SHUFFLE_SIZE)\n",
    "training_dataset = training_dataset.batch(BATCH_SIZE).repeat()\n",
    "\n",
    "validation_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_labels))\n",
    "validation_dataset = validation_dataset.shuffle(SHUFFLE_SIZE)\n",
    "validation_dataset = validation_dataset.batch(BATCH_SIZE).repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "jvDEbM1DVigY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 31 steps, validate for 31 steps\n",
      "Epoch 1/14\n",
      "31/31 [==============================] - 4s 114ms/step - loss: 1.4879 - accuracy: 0.4682 - val_loss: 0.9293 - val_accuracy: 0.6815\n",
      "Epoch 2/14\n",
      "31/31 [==============================] - 3s 81ms/step - loss: 0.8981 - accuracy: 0.6656 - val_loss: 0.8139 - val_accuracy: 0.7026\n",
      "Epoch 3/14\n",
      "31/31 [==============================] - 2s 78ms/step - loss: 0.7699 - accuracy: 0.6996 - val_loss: 0.7061 - val_accuracy: 0.7296\n",
      "Epoch 4/14\n",
      "31/31 [==============================] - 2s 76ms/step - loss: 0.6877 - accuracy: 0.7490 - val_loss: 0.6589 - val_accuracy: 0.7513\n",
      "Epoch 5/14\n",
      "31/31 [==============================] - 3s 82ms/step - loss: 0.6283 - accuracy: 0.7654 - val_loss: 0.6377 - val_accuracy: 0.7467\n",
      "Epoch 6/14\n",
      "31/31 [==============================] - 3s 88ms/step - loss: 0.6159 - accuracy: 0.7692 - val_loss: 0.5816 - val_accuracy: 0.7891\n",
      "Epoch 7/14\n",
      "31/31 [==============================] - 2s 79ms/step - loss: 0.5553 - accuracy: 0.7805 - val_loss: 0.5470 - val_accuracy: 0.7976\n",
      "Epoch 8/14\n",
      "31/31 [==============================] - 3s 93ms/step - loss: 0.5575 - accuracy: 0.7941 - val_loss: 0.5475 - val_accuracy: 0.7959\n",
      "Epoch 9/14\n",
      "31/31 [==============================] - 3s 108ms/step - loss: 0.5180 - accuracy: 0.8117 - val_loss: 0.5682 - val_accuracy: 0.7840\n",
      "Epoch 10/14\n",
      "31/31 [==============================] - 3s 103ms/step - loss: 0.5219 - accuracy: 0.8034 - val_loss: 0.5291 - val_accuracy: 0.8044\n",
      "Epoch 11/14\n",
      "31/31 [==============================] - 3s 99ms/step - loss: 0.4896 - accuracy: 0.8191 - val_loss: 0.5399 - val_accuracy: 0.8034\n",
      "Epoch 12/14\n",
      "31/31 [==============================] - 2s 77ms/step - loss: 0.4876 - accuracy: 0.8183 - val_loss: 0.4885 - val_accuracy: 0.8087\n",
      "Epoch 13/14\n",
      "31/31 [==============================] - 2s 78ms/step - loss: 0.4714 - accuracy: 0.8180 - val_loss: 0.4802 - val_accuracy: 0.8196\n",
      "Epoch 14/14\n",
      "31/31 [==============================] - 2s 76ms/step - loss: 0.4488 - accuracy: 0.8347 - val_loss: 0.4577 - val_accuracy: 0.8286\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x13e569b90>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EPOCHS = 14\n",
    "\n",
    "model.fit(training_dataset, \n",
    "    validation_data=validation_dataset,\n",
    "    steps_per_epoch = 1000 // 32,\n",
    "    epochs=EPOCHS,\n",
    "    validation_steps=1000 // 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again to evaluate the model we need to check the accuracy on unseen or test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "O5ddqxgdVzQP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Model  Loss: 0.458993\tAccuracy: 0.831500\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=0)\n",
    "print('\\nTest Model  Loss: %.6f\\tAccuracy: %.6f' % (test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "5-conv-draft.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
